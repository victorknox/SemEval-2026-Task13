{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":113636,"databundleVersionId":14080332,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade transformers==4.45.0 huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:28:41.954056Z","iopub.execute_input":"2025-10-28T16:28:41.954288Z","iopub.status.idle":"2025-10-28T16:28:57.561770Z","shell.execute_reply.started":"2025-10-28T16:28:41.954267Z","shell.execute_reply":"2025-10-28T16:28:57.561006Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.45.0\n  Downloading transformers-4.45.0-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (1.0.0rc2)\nCollecting huggingface_hub\n  Downloading huggingface_hub-1.0.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.0) (3.19.1)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.0) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.0) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.0) (2.32.5)\nCollecting tokenizers<0.21,>=0.20 (from transformers==4.45.0)\n  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.0) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.0) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.45.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.45.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.45.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.45.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.45.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.45.0) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.0) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.0) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.45.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.45.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.45.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.45.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.45.0) (2024.2.0)\nDownloading transformers-4.45.0-py3-none-any.whl (9.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface_hub, tokenizers, transformers\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface_hub-0.36.0 tokenizers-0.20.3 transformers-4.45.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    RobertaTokenizer,\n    RobertaForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    EarlyStoppingCallback,\n    DataCollatorWithPadding\n)\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\nimport argparse\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:28:57.562936Z","iopub.execute_input":"2025-10-28T16:28:57.563246Z","execution_failed":"2025-10-28T16:29:00.222Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We suggest using a single class, it will make refinement easier. \n\nIn your implementation, feel free to update the training procedure, change model and do whatever feels right ","metadata":{}},{"cell_type":"code","source":"class CodeBERTTrainer:\n    def __init__(self, max_length=512, model_name=\"microsoft/codebert-base\"):\n        self.max_length = max_length\n        self.model_name = model_name\n        self.tokenizer = None\n        self.model = None\n        self.num_labels = None\n        \n    def load_and_prepare_data(self):\n        \n        try:\n            df = pd.read_parquet('/kaggle/input/sem-eval-2026-task-13-subtask-a/Task_A/train.parquet')\n            \n            print(f\"Dataset columns: {df.columns.tolist()}\")\n            print(f\"Sample data:\\n{df.head()}\")\n            \n            if 'code' not in df.columns or 'label' not in df.columns:\n                raise ValueError(\"Dataset must contain 'code' and 'label' columns\")\n            \n            df = df.dropna(subset=['code', 'label'])\n            \n            df['label'] = df['label'].astype(int)\n            self.num_labels = df['label'].nunique()\n            \n            print(f\"Number of unique labels: {self.num_labels}\")\n            print(f\"Label range: {df['label'].min()} to {df['label'].max()}\")\n            print(f\"Label distribution:\\n{df['label'].value_counts().sort_index()}\")\n\n            val_df = pd.read_parquet('/kaggle/input/sem-eval-2026-task-13-subtask-a/Task_A/validation.parquet')\n            \n            print(f\"Train samples: {len(df)}, Validation samples: {len(val_df)}\")\n            \n            return df, val_df\n            \n        except Exception as e:\n            print(f\"Error loading dataset: {e}\")\n            raise\n    \n    def initialize_model_and_tokenizer(self):\n        print(f\"Initializing {self.model_name} model and tokenizer...\")\n        \n        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_name)\n        \n        self.model = RobertaForSequenceClassification.from_pretrained(\n            self.model_name,\n            num_labels=self.num_labels,\n            problem_type=\"single_label_classification\"\n        ).to('cuda')\n        \n        print(f\"Model initialized with {self.num_labels} labels\")\n    \n    def tokenize_function(self, examples):\n        return self.tokenizer(\n            examples['code'],\n            truncation=True,\n            padding=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n    \n    def prepare_datasets(self, train_df, val_df):\n        print(\"Preparing datasets for training...\")\n        \n        train_dataset = Dataset.from_pandas(train_df[['code', 'label']])\n        val_dataset = Dataset.from_pandas(val_df[['code', 'label']])\n        \n        train_dataset = train_dataset.map(\n            self.tokenize_function,\n            batched=True,\n            remove_columns=['code']\n        )\n        val_dataset = val_dataset.map(\n            self.tokenize_function,\n            batched=True,\n            remove_columns=['code']\n        )\n        \n        train_dataset = train_dataset.rename_column('label', 'labels')\n        val_dataset = val_dataset.rename_column('label', 'labels')\n        \n        return train_dataset, val_dataset\n    \n    def compute_metrics(self, eval_pred):\n        predictions, labels = eval_pred\n        predictions = np.argmax(predictions, axis=1)\n        \n        accuracy = accuracy_score(labels, predictions)\n        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n        \n        return {\n            'accuracy': accuracy,\n            'f1': f1,\n            'precision': precision,\n            'recall': recall\n        }\n    \n    def train(self, train_dataset, val_dataset, output_dir=\"./results\", num_epochs=3, batch_size=16, learning_rate=2e-5):\n        print(\"Starting training...\")\n        print(self.model)\n        print(self.model.device)\n        training_args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=num_epochs,\n            per_device_train_batch_size=batch_size,\n            per_device_eval_batch_size=batch_size,\n            # warmup_steps=500,\n            weight_decay=0.01,\n            logging_dir='./logs',\n            logging_steps=5,\n            evaluation_strategy=\"steps\",\n            eval_steps=500,\n            save_strategy=\"steps\",\n            save_steps=500,\n            load_best_model_at_end=True,\n            metric_for_best_model=\"f1\",\n            greater_is_better=True,\n            remove_unused_columns=False,\n            learning_rate=learning_rate,\n            lr_scheduler_type=\"linear\",\n            save_total_limit=2,\n            report_to=[],\n        )\n        \n        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n        \n        trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset=val_dataset,\n            tokenizer=self.tokenizer,\n            data_collator=data_collator,\n            compute_metrics=self.compute_metrics,\n            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n        )\n        print(f\"Start training\")\n        trainer.train()\n        \n        trainer.save_model()\n        self.tokenizer.save_pretrained(output_dir)\n        \n        print(f\"Training completed. Model saved to {output_dir}\")\n        \n        return trainer\n    \n    def evaluate_model(self, trainer, val_dataset):\n        print(\"Evaluating model...\")\n        \n        predictions = trainer.predict(val_dataset)\n        y_pred = np.argmax(predictions.predictions, axis=1)\n        y_true = predictions.label_ids\n        \n        print(\"Classification Report:\")\n        print(classification_report(y_true, y_pred))\n        \n        return predictions\n    \n    def run_full_pipeline(self, output_dir=\"./results\", num_epochs=3, batch_size=16, learning_rate=2e-5):\n        try:\n            train_df, val_df = self.load_and_prepare_data()\n            \n            self.initialize_model_and_tokenizer()\n            \n            train_dataset, val_dataset = self.prepare_datasets(train_df, val_df)\n            \n            trainer = self.train(\n                train_dataset, val_dataset, \n                output_dir=output_dir,\n                num_epochs=num_epochs,\n                batch_size=batch_size,\n                learning_rate=learning_rate\n            )\n            \n            self.evaluate_model(trainer, val_dataset)\n            \n            print(\"Pipeline completed successfully!\")\n            return trainer\n            \n        except Exception as e:\n            print(f\"Error in pipeline: {e}\")\n            raise\n    ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-28T16:29:00.223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"OUTPUT_DIR = \"taskA-model\"\n\ntrainer_obj = CodeBERTTrainer(\n    max_length=256, \n)\n\nt = trainer_obj.run_full_pipeline(\n    output_dir=OUTPUT_DIR,\n    num_epochs=10,\n    batch_size=16,\n    learning_rate=2e-5\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport logging\nfrom itertools import chain\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n\n@torch.no_grad()\ndef predict_with_trainer(trainer_obj, parquet_path, output_path, max_length=512, batch_size=16, device=None):\n    \"\"\"\n    Uses trainer_obj.model and trainer_obj.tokenizer to run streaming inference\n    over a parquet file with columns ['ID','code'] and writes 'ID,prediction' CSV.\n    \"\"\"\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Pull model & tokenizer from your trainer object\n    model = trainer_obj.model\n    tokenizer = trainer_obj.tokenizer if hasattr(trainer_obj, \"tokenizer\") else trainer_obj.args._setup_devices and None\n    if tokenizer is None and hasattr(trainer_obj, \"tokenizer\"):\n        tokenizer = trainer_obj.tokenizer\n    if tokenizer is None:\n        raise ValueError(\"trainer_obj must have a tokenizer (e.g., provided when creating the Trainer).\")\n\n    model.to(device)\n    model.eval()\n\n    # Stream parquet (no RAM blowup)\n    ds = load_dataset(\"parquet\", data_files=parquet_path, split=\"train\", streaming=True)\n\n    # Validate schema and re-chain the first row back into the stream\n    it = iter(ds)\n    first = next(it)\n    if not {\"ID\", \"code\"}.issubset(first.keys()):\n        raise ValueError(\"Parquet file must contain 'ID' and 'code' columns\")\n    stream = chain([first], it)\n\n    def batcher(iterator, bs):\n        buf = []\n        for ex in iterator:\n            buf.append(ex)\n            if len(buf) == bs:\n                yield buf\n                buf = []\n        if buf:\n            yield buf\n\n    with open(output_path, \"w\") as f:\n        f.write(\"ID,prediction\\n\")\n\n        for batch in tqdm(batcher(stream, batch_size), desc=\"Predicting\"):\n            codes = [row[\"code\"] for row in batch]\n            ids   = [row[\"ID\"] for row in batch]\n\n            enc = tokenizer(\n                codes,\n                truncation=True,\n                padding=True,\n                max_length=max_length,\n                return_tensors=\"pt\",\n            )\n            input_ids = enc[\"input_ids\"].to(device)\n            attention_mask = enc[\"attention_mask\"].to(device)\n\n            logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n            pred_labels = logits.argmax(dim=-1).cpu().tolist()\n\n            for ex_id, pred in zip(ids, pred_labels):\n                f.write(f\"{ex_id},{pred}\\n\")\n\n    print(f\"Predictions saved to {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:26:42.590188Z","iopub.execute_input":"2025-10-28T16:26:42.590398Z","iopub.status.idle":"2025-10-28T16:26:42.599610Z","shell.execute_reply.started":"2025-10-28T16:26:42.590381Z","shell.execute_reply":"2025-10-28T16:26:42.598853Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# After training:\n# trainer_obj = CodeBERTTrainer(...).run_full_pipeline(output_dir=..., ...)\n\nTEST_PARQUET = \"/kaggle/input/sem-eval-2026-task-13-subtask-a/Task_A/test.parquet\"  # adjust if needed\nOUT_CSV = \"/kaggle/working/submission.csv\"\n\npredict_with_trainer(\n    trainer_obj=t,          \n    parquet_path=TEST_PARQUET,\n    output_path=OUT_CSV,\n    max_length=256,\n    batch_size=32,\n    device=\"cuda\"              \n)\n\nprint(\"Wrote:\", OUT_CSV)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:26:43.542200Z","iopub.execute_input":"2025-10-28T16:26:43.542698Z","iopub.status.idle":"2025-10-28T16:26:53.483611Z","shell.execute_reply.started":"2025-10-28T16:26:43.542674Z","shell.execute_reply":"2025-10-28T16:26:53.482655Z"}},"outputs":[{"name":"stderr","text":"Predicting: 32it [00:09,  3.35it/s]","output_type":"stream"},{"name":"stdout","text":"Predictions saved to /kaggle/working/submission.csv\nWrote: /kaggle/working/submission.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}